"""
Complete RAG + LLM Chatbot Service
ë²¡í„° ê²€ìƒ‰ + OpenRouter LLMì„ ê²°í•©í•œ ì™„ì „í•œ ì±—ë´‡
"""
from typing import List, Dict, Optional
import logging
from .rag_simple import get_simple_rag_service, SimpleRAGService
from .openrouter import get_openrouter_client, OpenRouterClient
from .prompts import PromptTemplates

logger = logging.getLogger(__name__)


class ChatbotService:
    """
    ì™„ì „í•œ RAG ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤
    """
    
    def __init__(self):
        self.rag_service: Optional[SimpleRAGService] = None
        self.llm_client: Optional[OpenRouterClient] = None
        self._initialized = False
        
    def initialize(self):
        """ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        try:
            logger.info("ğŸš€ ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            
            # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            self.rag_service = get_simple_rag_service()
            self.rag_service.initialize()
            
            # OpenRouter í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.llm_client = get_openrouter_client()
            
            self._initialized = True
            logger.info("âœ… ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def generate_response(
        self,
        user_query: str,
        dbti_type: Optional[str] = None,
        conversation_history: Optional[List[Dict]] = None,
        use_llm: bool = True,
        model: Optional[str] = None
    ) -> Dict:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            dbti_type: ì‚¬ìš©ìì˜ DBTI ìœ í˜•
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€ (Falseë©´ RAGë§Œ ì‚¬ìš©)
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            
        Returns:
            Dict: ì‘ë‹µ ë° ë©”íƒ€ë°ì´í„°
        """
        if not self._initialized:
            self.initialize()
        
        try:
            # 1. RAG: ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {user_query[:50]}...")
            
            context_docs = self.rag_service.retrieve_context(
                query=user_query,
                top_k=3,
                dbti_filter=dbti_type,
                min_score=0.3
            )
            
            # 2. LLM ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì‘ë‹µ ìƒì„±
            if use_llm and self.llm_client.api_key:
                # OpenRouter LLM ì‚¬ìš©
                response_text = await self._generate_llm_response(
                    user_query=user_query,
                    context_docs=context_docs,
                    dbti_type=dbti_type,
                    conversation_history=conversation_history,
                    model=model
                )
                method = "RAG + LLM"
            else:
                # RAGë§Œ ì‚¬ìš©
                logger.info("â„¹ï¸  LLM ë¯¸ì‚¬ìš© - RAG ê¸°ë°˜ ì‘ë‹µ")
                rag_result = self.rag_service.generate_response_with_context(
                    query=user_query,
                    dbti_type=dbti_type,
                    top_k=3
                )
                response_text = rag_result["response"]
                method = "RAG only"
            
            return {
                "response": response_text,
                "sources": [doc["title"] for doc in context_docs],
                "num_sources": len(context_docs),
                "confidence": context_docs[0]["score"] if context_docs else 0.0,
                "method": method,
                "dbti_type": dbti_type
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            
            # í´ë°±: ê°„ë‹¨í•œ ì‘ë‹µ
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "num_sources": 0,
                "confidence": 0.0,
                "method": "fallback",
                "error": str(e)
            }
    
    async def _generate_llm_response(
        self,
        user_query: str,
        context_docs: List[Dict],
        dbti_type: Optional[str],
        conversation_history: Optional[List[Dict]],
        model: Optional[str]
    ) -> str:
        """
        OpenRouter LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ
            dbti_type: DBTI ìœ í˜•
            conversation_history: ëŒ€í™” ê¸°ë¡
            model: LLM ëª¨ë¸
            
        Returns:
            str: LLM ìƒì„± ì‘ë‹µ
        """
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = PromptTemplates.create_conversation_messages(
            user_query=user_query,
            context_documents=context_docs,
            dbti_type=dbti_type,
            conversation_history=conversation_history
        )
        
        logger.info(f"ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì¤‘... (ëª¨ë¸: {model or 'ê¸°ë³¸'})")
        
        # OpenRouter API í˜¸ì¶œ
        result = await self.llm_client.chat_completion(
            messages=messages,
            model=model,
            temperature=0.7,
            max_tokens=1000
        )
        
        # ì‘ë‹µ ì¶”ì¶œ
        if "choices" in result and len(result["choices"]) > 0:
            response_text = result["choices"][0]["message"]["content"]
            logger.info(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(response_text)}ì)")
            return response_text
        else:
            logger.error("âŒ LLM ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise ValueError("Invalid LLM response format")
    
    async def explain_dbti_type(
        self,
        dbti_code: str,
        use_llm: bool = True
    ) -> Dict:
        """
        DBTI ìœ í˜•ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„±
        
        Args:
            dbti_code: DBTI ì½”ë“œ (ì˜ˆ: WTIL)
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            Dict: ì„¤ëª… ë° ë©”íƒ€ë°ì´í„°
        """
        if not self._initialized:
            self.initialize()
        
        # í•´ë‹¹ DBTI ìœ í˜•ì˜ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰
        dbti_docs = self.rag_service.search_by_dbti(dbti_code, top_k=10)
        
        if not dbti_docs:
            return {
                "response": f"{dbti_code} ìœ í˜•ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0
            }
        
        if use_llm and self.llm_client.api_key:
            # LLMìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª… ìƒì„±
            prompt = PromptTemplates.create_dbti_explanation_prompt(
                dbti_code, dbti_docs
            )
            
            messages = [
                {"role": "system", "content": PromptTemplates.system_prompt(dbti_code)},
                {"role": "user", "content": prompt}
            ]
            
            result = await self.llm_client.chat_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=1500
            )
            
            response_text = result["choices"][0]["message"]["content"]
        else:
            # RAGë§Œ ì‚¬ìš©
            response_text = f"**{dbti_code} ìœ í˜• ì •ë³´**:\n\n"
            for doc in dbti_docs[:5]:
                response_text += f"â€¢ {doc['title']}\n{doc['content']}\n\n"
        
        return {
            "response": response_text,
            "sources": [doc["title"] for doc in dbti_docs[:5]],
            "confidence": 1.0,
            "dbti_code": dbti_code
        }
    
    def get_status(self) -> Dict:
        """ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        return {
            "initialized": self._initialized,
            "rag_available": self.rag_service is not None,
            "llm_available": self.llm_client is not None and bool(self.llm_client.api_key),
            "mode": "RAG + LLM" if (self.llm_client and self.llm_client.api_key) else "RAG only"
        }


# ì „ì—­ ì±—ë´‡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_chatbot_service: Optional[ChatbotService] = None


def get_chatbot_service() -> ChatbotService:
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _chatbot_service
    if _chatbot_service is None:
        _chatbot_service = ChatbotService()
    return _chatbot_service

