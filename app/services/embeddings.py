"""
Embedding Service using Sentence Transformers
í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„œë¹„ìŠ¤
"""
from sentence_transformers import SentenceTransformer
from typing import List, Optional
import numpy as np
import logging

logger = logging.getLogger(__name__)


class EmbeddingService:
    """
    Sentence Transformersë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤
    """
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Args:
            model_name: Sentence Transformers ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        self.model: Optional[SentenceTransformer] = None
        self.embedding_dim = 384  # all-MiniLM-L6-v2ì˜ ì°¨ì›
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (lazy loading)"""
        if self.model is None:
            logger.info(f"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
            try:
                self.model = SentenceTransformer(self.model_name)
                logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.embedding_dim})")
            except Exception as e:
                logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
    
    def encode_text(self, text: str) -> np.ndarray:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            np.ndarray: ì„ë² ë”© ë²¡í„° (384ì°¨ì›)
        """
        if self.model is None:
            self.load_model()
        
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding
    
    def encode_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²¡í„° ë³€í™˜ (íš¨ìœ¨ì )
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            np.ndarray: ì„ë² ë”© ë²¡í„° ë°°ì—´ (N x 384)
        """
        if self.model is None:
            self.load_model()
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            convert_to_numpy=True,
            show_progress_bar=len(texts) > 100
        )
        return embeddings
    
    def compute_similarity(self, text1: str, text2: str) -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        
        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            
        Returns:
            float: ìœ ì‚¬ë„ (0~1)
        """
        emb1 = self.encode_text(text1)
        emb2 = self.encode_text(text2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity)


# ì „ì—­ ì„ë² ë”© ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service(model_name: str = "sentence-transformers/all-MiniLM-L6-v2") -> EmbeddingService:
    """
    ì„ë² ë”© ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    """
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService(model_name)
    return _embedding_service

