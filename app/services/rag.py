"""
RAG (Retrieval-Augmented Generation) Service
ë²¡í„° ê²€ìƒ‰ + LLMì„ ê²°í•©í•œ ì§€ì‹ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
"""
from typing import List, Dict, Optional
import logging
from .embeddings import get_embedding_service
from .vector_db import get_vector_db_service

logger = logging.getLogger(__name__)


class RAGService:
    """
    RAG ì„œë¹„ìŠ¤: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³  ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    """
    
    def __init__(self):
        self.embedding_service = get_embedding_service()
        self.vector_db = get_vector_db_service()
        self._initialized = False
    
    def initialize(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        try:
            logger.info("ğŸš€ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            self.embedding_service.load_model()
            
            # ë²¡í„° DB ì—°ê²° ë° ì»¬ë ‰ì…˜ ë¡œë“œ
            self.vector_db.connect()
            self.vector_db.create_collection()
            
            self._initialized = True
            logger.info("âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def retrieve_context(
        self,
        query: str,
        top_k: int = 5,
        pawna_filter: Optional[str] = None,
        min_score: float = 0.3
    ) -> List[Dict]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            pawna_filter: Pawna ì½”ë“œë¡œ í•„í„°ë§
            min_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜
            
        Returns:
            List[Dict]: ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self._initialized:
            self.initialize()
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_service.encode_text(query)
        
        # ë²¡í„° ê²€ìƒ‰
        results = self.vector_db.search(
            query_embedding=query_embedding.tolist(),
            top_k=top_k,
            pawna_filter=pawna_filter
        )
        
        # ìµœì†Œ ì ìˆ˜ í•„í„°ë§
        filtered_results = [r for r in results if r['score'] >= min_score]
        
        logger.info(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}/{len(results)}ê°œ ë¬¸ì„œ (ìµœì†Œ ì ìˆ˜: {min_score})")
        
        return filtered_results
    
    def format_context(self, retrieved_docs: List[Dict]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        
        Args:
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not retrieved_docs:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            context_parts.append(
                f"[ë¬¸ì„œ {i}] {doc['title']}\n"
                f"{doc['content']}\n"
                f"(Pawna: {doc['pawna_code']}, ìœ ì‚¬ë„: {doc['score']:.2f})"
            )
        
        return "\n\n".join(context_parts)
    
    def generate_response_with_context(
        self,
        query: str,
        pawna_type: Optional[str] = None,
        top_k: int = 3
    ) -> Dict:
        """
        RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            pawna_type: ì‚¬ìš©ìì˜ Pawna ìœ í˜• (ì»¨í…ìŠ¤íŠ¸)
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            Dict: ì‘ë‹µ ë° ë©”íƒ€ë°ì´í„°
        """
        # 1. ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        retrieved_docs = self.retrieve_context(
            query=query,
            top_k=top_k,
            pawna_filter=pawna_type
        )
        
        # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context = self.format_context(retrieved_docs)
        
        # 3. ì‘ë‹µ ìƒì„±
        # í˜„ì¬ëŠ” ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨í•œ ì‘ë‹µ
        if retrieved_docs:
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì‚¬ìš©
            top_doc = retrieved_docs[0]
            response = f"{top_doc['content']}"
            
            # Pawna ìœ í˜• ì •ë³´ ì¶”ê°€
            if pawna_type:
                response += f"\n\nğŸ’¡ {pawna_type} ìœ í˜•ì— ëŒ€í•œ ë§ì¶¤ ì •ë³´ì…ë‹ˆë‹¤."
        else:
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
        
        return {
            "response": response,
            "context": context,
            "sources": [doc['title'] for doc in retrieved_docs],
            "num_sources": len(retrieved_docs),
            "confidence": retrieved_docs[0]['score'] if retrieved_docs else 0.0
        }
    
    def search_similar_questions(
        self,
        question: str,
        top_k: int = 5
    ) -> List[Dict]:
        """
        ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰ (FAQ ê¸°ëŠ¥)
        
        Args:
            question: ê²€ìƒ‰í•  ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            List[Dict]: ìœ ì‚¬í•œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        if not self._initialized:
            self.initialize()
        
        # ì§ˆë¬¸ ì„ë² ë”©
        query_embedding = self.embedding_service.encode_text(question)
        
        # QA ì¹´í…Œê³ ë¦¬ë§Œ ê²€ìƒ‰
        results = self.vector_db.search(
            query_embedding=query_embedding.tolist(),
            top_k=top_k
        )
        
        # QA ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        qa_results = [r for r in results if r.get('category') == 'qa']
        
        return qa_results


# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_rag_service: Optional[RAGService] = None


def get_rag_service() -> RAGService:
    """RAG ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service

